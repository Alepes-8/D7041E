{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "\n",
    "def f_relu(x,deriv=False):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_with_err(err_str):\n",
    "    print(err_str, file=sys.stderr)\n",
    "    #sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=f_sigmoid))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=f_relu))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "            #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            \n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                \n",
    "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "Xtr = Xtr.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "Xtr = Xtr.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "Xtr /= 255\n",
    "X_test /= 255\n",
    "print(Xtr.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.64270 Test error: 0.64800\n",
      "[   1]  Training error: 0.42200 Test error: 0.42700\n",
      "[   2]  Training error: 0.12330 Test error: 0.11980\n",
      "[   3]  Training error: 0.08178 Test error: 0.08130\n",
      "[   4]  Training error: 0.06572 Test error: 0.06780\n",
      "[   5]  Training error: 0.05490 Test error: 0.05950\n",
      "[   6]  Training error: 0.04615 Test error: 0.05140\n",
      "[   7]  Training error: 0.03942 Test error: 0.04500\n",
      "[   8]  Training error: 0.03363 Test error: 0.03960\n",
      "[   9]  Training error: 0.02938 Test error: 0.03630\n",
      "[  10]  Training error: 0.02573 Test error: 0.03300\n",
      "[  11]  Training error: 0.02222 Test error: 0.03200\n",
      "[  12]  Training error: 0.01968 Test error: 0.03100\n",
      "[  13]  Training error: 0.01723 Test error: 0.03000\n",
      "[  14]  Training error: 0.01550 Test error: 0.02850\n",
      "[  15]  Training error: 0.01402 Test error: 0.02760\n",
      "[  16]  Training error: 0.01300 Test error: 0.02720\n",
      "[  17]  Training error: 0.01210 Test error: 0.02710\n",
      "[  18]  Training error: 0.01142 Test error: 0.02640\n",
      "[  19]  Training error: 0.01047 Test error: 0.02620\n",
      "[  20]  Training error: 0.00928 Test error: 0.02560\n",
      "[  21]  Training error: 0.00847 Test error: 0.02620\n",
      "[  22]  Training error: 0.00737 Test error: 0.02610\n",
      "[  23]  Training error: 0.00650 Test error: 0.02590\n",
      "[  24]  Training error: 0.00578 Test error: 0.02550\n",
      "[  25]  Training error: 0.00513 Test error: 0.02500\n",
      "[  26]  Training error: 0.00448 Test error: 0.02540\n",
      "[  27]  Training error: 0.00418 Test error: 0.02510\n",
      "[  28]  Training error: 0.00383 Test error: 0.02520\n",
      "[  29]  Training error: 0.00347 Test error: 0.02500\n",
      "[  30]  Training error: 0.00300 Test error: 0.02500\n",
      "[  31]  Training error: 0.00268 Test error: 0.02490\n",
      "[  32]  Training error: 0.00242 Test error: 0.02480\n",
      "[  33]  Training error: 0.00220 Test error: 0.02420\n",
      "[  34]  Training error: 0.00198 Test error: 0.02440\n",
      "[  35]  Training error: 0.00180 Test error: 0.02490\n",
      "[  36]  Training error: 0.00168 Test error: 0.02480\n",
      "[  37]  Training error: 0.00158 Test error: 0.02490\n",
      "[  38]  Training error: 0.00150 Test error: 0.02490\n",
      "[  39]  Training error: 0.00143 Test error: 0.02470\n",
      "[  40]  Training error: 0.00133 Test error: 0.02470\n",
      "[  41]  Training error: 0.00135 Test error: 0.02460\n",
      "[  42]  Training error: 0.00130 Test error: 0.02440\n",
      "[  43]  Training error: 0.00127 Test error: 0.02430\n",
      "[  44]  Training error: 0.00127 Test error: 0.02450\n",
      "[  45]  Training error: 0.00123 Test error: 0.02420\n",
      "[  46]  Training error: 0.00128 Test error: 0.02490\n",
      "[  47]  Training error: 0.00107 Test error: 0.02500\n",
      "[  48]  Training error: 0.00068 Test error: 0.02440\n",
      "[  49]  Training error: 0.00047 Test error: 0.02430\n",
      "[  50]  Training error: 0.00032 Test error: 0.02440\n",
      "[  51]  Training error: 0.00025 Test error: 0.02430\n",
      "[  52]  Training error: 0.00022 Test error: 0.02420\n",
      "[  53]  Training error: 0.00020 Test error: 0.02420\n",
      "[  54]  Training error: 0.00017 Test error: 0.02420\n",
      "[  55]  Training error: 0.00017 Test error: 0.02420\n",
      "[  56]  Training error: 0.00017 Test error: 0.02420\n",
      "[  57]  Training error: 0.00017 Test error: 0.02400\n",
      "[  58]  Training error: 0.00017 Test error: 0.02430\n",
      "[  59]  Training error: 0.00012 Test error: 0.02410\n",
      "[  60]  Training error: 0.00010 Test error: 0.02410\n",
      "[  61]  Training error: 0.00008 Test error: 0.02410\n",
      "[  62]  Training error: 0.00008 Test error: 0.02410\n",
      "[  63]  Training error: 0.00008 Test error: 0.02420\n",
      "[  64]  Training error: 0.00007 Test error: 0.02410\n",
      "[  65]  Training error: 0.00005 Test error: 0.02410\n",
      "[  66]  Training error: 0.00005 Test error: 0.02400\n",
      "[  67]  Training error: 0.00005 Test error: 0.02420\n",
      "[  68]  Training error: 0.00005 Test error: 0.02410\n",
      "[  69]  Training error: 0.00005 Test error: 0.02440\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,eta=0.05,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.70332 Test error: 0.70080\n",
      "[   1]  Training error: 0.64722 Test error: 0.64300\n",
      "[   2]  Training error: 0.59978 Test error: 0.59850\n",
      "[   3]  Training error: 0.44783 Test error: 0.45720\n",
      "[   4]  Training error: 0.21120 Test error: 0.20110\n",
      "[   5]  Training error: 0.11015 Test error: 0.10790\n",
      "[   6]  Training error: 0.08857 Test error: 0.08650\n",
      "[   7]  Training error: 0.07573 Test error: 0.07480\n",
      "[   8]  Training error: 0.06440 Test error: 0.06470\n",
      "[   9]  Training error: 0.05615 Test error: 0.05700\n",
      "[  10]  Training error: 0.04932 Test error: 0.05200\n",
      "[  11]  Training error: 0.04377 Test error: 0.04750\n",
      "[  12]  Training error: 0.04005 Test error: 0.04480\n",
      "[  13]  Training error: 0.03602 Test error: 0.04160\n",
      "[  14]  Training error: 0.03283 Test error: 0.03850\n",
      "[  15]  Training error: 0.02958 Test error: 0.03620\n",
      "[  16]  Training error: 0.02717 Test error: 0.03380\n",
      "[  17]  Training error: 0.02488 Test error: 0.03210\n",
      "[  18]  Training error: 0.02287 Test error: 0.03100\n",
      "[  19]  Training error: 0.02128 Test error: 0.02960\n",
      "[  20]  Training error: 0.01963 Test error: 0.02760\n",
      "[  21]  Training error: 0.01842 Test error: 0.02710\n",
      "[  22]  Training error: 0.01718 Test error: 0.02680\n",
      "[  23]  Training error: 0.01605 Test error: 0.02640\n",
      "[  24]  Training error: 0.01488 Test error: 0.02560\n",
      "[  25]  Training error: 0.01408 Test error: 0.02510\n",
      "[  26]  Training error: 0.01320 Test error: 0.02520\n",
      "[  27]  Training error: 0.01235 Test error: 0.02500\n",
      "[  28]  Training error: 0.01163 Test error: 0.02530\n",
      "[  29]  Training error: 0.01105 Test error: 0.02540\n",
      "[  30]  Training error: 0.01035 Test error: 0.02550\n",
      "[  31]  Training error: 0.00968 Test error: 0.02560\n",
      "[  32]  Training error: 0.00923 Test error: 0.02590\n",
      "[  33]  Training error: 0.00863 Test error: 0.02560\n",
      "[  34]  Training error: 0.00813 Test error: 0.02540\n",
      "[  35]  Training error: 0.00770 Test error: 0.02550\n",
      "[  36]  Training error: 0.00713 Test error: 0.02540\n",
      "[  37]  Training error: 0.00682 Test error: 0.02530\n",
      "[  38]  Training error: 0.00650 Test error: 0.02550\n",
      "[  39]  Training error: 0.00625 Test error: 0.02510\n",
      "[  40]  Training error: 0.00585 Test error: 0.02520\n",
      "[  41]  Training error: 0.00548 Test error: 0.02530\n",
      "[  42]  Training error: 0.00505 Test error: 0.02560\n",
      "[  43]  Training error: 0.00475 Test error: 0.02510\n",
      "[  44]  Training error: 0.00433 Test error: 0.02500\n",
      "[  45]  Training error: 0.00407 Test error: 0.02500\n",
      "[  46]  Training error: 0.00367 Test error: 0.02500\n",
      "[  47]  Training error: 0.00323 Test error: 0.02510\n",
      "[  48]  Training error: 0.00293 Test error: 0.02520\n",
      "[  49]  Training error: 0.00247 Test error: 0.02500\n",
      "[  50]  Training error: 0.00212 Test error: 0.02510\n",
      "[  51]  Training error: 0.00192 Test error: 0.02510\n",
      "[  52]  Training error: 0.00175 Test error: 0.02510\n",
      "[  53]  Training error: 0.00155 Test error: 0.02500\n",
      "[  54]  Training error: 0.00138 Test error: 0.02490\n",
      "[  55]  Training error: 0.00123 Test error: 0.02480\n",
      "[  56]  Training error: 0.00113 Test error: 0.02460\n",
      "[  57]  Training error: 0.00098 Test error: 0.02460\n",
      "[  58]  Training error: 0.00090 Test error: 0.02440\n",
      "[  59]  Training error: 0.00080 Test error: 0.02430\n",
      "[  60]  Training error: 0.00072 Test error: 0.02440\n",
      "[  61]  Training error: 0.00068 Test error: 0.02440\n",
      "[  62]  Training error: 0.00058 Test error: 0.02420\n",
      "[  63]  Training error: 0.00047 Test error: 0.02430\n",
      "[  64]  Training error: 0.00040 Test error: 0.02430\n",
      "[  65]  Training error: 0.00030 Test error: 0.02410\n",
      "[  66]  Training error: 0.00028 Test error: 0.02440\n",
      "[  67]  Training error: 0.00027 Test error: 0.02420\n",
      "[  68]  Training error: 0.00023 Test error: 0.02430\n",
      "[  69]  Training error: 0.00023 Test error: 0.02440\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels, eta=0.005 ,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.90248 Test error: 0.90260\n",
      "[   1]  Training error: 0.89558 Test error: 0.89720\n",
      "[   2]  Training error: 0.89782 Test error: 0.89900\n",
      "[   3]  Training error: 0.88763 Test error: 0.88650\n",
      "[   4]  Training error: 0.89782 Test error: 0.89900\n",
      "[   5]  Training error: 0.90248 Test error: 0.90260\n",
      "[   6]  Training error: 0.90248 Test error: 0.90260\n",
      "[   7]  Training error: 0.90137 Test error: 0.90420\n",
      "[   8]  Training error: 0.88763 Test error: 0.88650\n",
      "[   9]  Training error: 0.89782 Test error: 0.89900\n",
      "[  10]  Training error: 0.90085 Test error: 0.89910\n",
      "[  11]  Training error: 0.88763 Test error: 0.88650\n",
      "[  12]  Training error: 0.90263 Test error: 0.90180\n",
      "[  13]  Training error: 0.89782 Test error: 0.89900\n",
      "[  14]  Training error: 0.90248 Test error: 0.90260\n",
      "[  15]  Training error: 0.90085 Test error: 0.89910\n",
      "[  16]  Training error: 0.90070 Test error: 0.89680\n",
      "[  17]  Training error: 0.90085 Test error: 0.89910\n",
      "[  18]  Training error: 0.90137 Test error: 0.90420\n",
      "[  19]  Training error: 0.90248 Test error: 0.90260\n",
      "[  20]  Training error: 0.90248 Test error: 0.90260\n",
      "[  21]  Training error: 0.89558 Test error: 0.89720\n",
      "[  22]  Training error: 0.90965 Test error: 0.91080\n",
      "[  23]  Training error: 0.89782 Test error: 0.89900\n",
      "[  24]  Training error: 0.88763 Test error: 0.88650\n",
      "[  25]  Training error: 0.89782 Test error: 0.89900\n",
      "[  26]  Training error: 0.90248 Test error: 0.90260\n",
      "[  27]  Training error: 0.90263 Test error: 0.90180\n",
      "[  28]  Training error: 0.90248 Test error: 0.90260\n",
      "[  29]  Training error: 0.90128 Test error: 0.90200\n",
      "[  30]  Training error: 0.90128 Test error: 0.90200\n",
      "[  31]  Training error: 0.90965 Test error: 0.91080\n",
      "[  32]  Training error: 0.90070 Test error: 0.89680\n",
      "[  33]  Training error: 0.89782 Test error: 0.89900\n",
      "[  34]  Training error: 0.88763 Test error: 0.88650\n",
      "[  35]  Training error: 0.90085 Test error: 0.89910\n",
      "[  36]  Training error: 0.90137 Test error: 0.90420\n",
      "[  37]  Training error: 0.90248 Test error: 0.90260\n",
      "[  38]  Training error: 0.90137 Test error: 0.90420\n",
      "[  39]  Training error: 0.90248 Test error: 0.90260\n",
      "[  40]  Training error: 0.90085 Test error: 0.89910\n",
      "[  41]  Training error: 0.90137 Test error: 0.90420\n",
      "[  42]  Training error: 0.90248 Test error: 0.90260\n",
      "[  43]  Training error: 0.90263 Test error: 0.90180\n",
      "[  44]  Training error: 0.89782 Test error: 0.89900\n",
      "[  45]  Training error: 0.89782 Test error: 0.89900\n",
      "[  46]  Training error: 0.89558 Test error: 0.89720\n",
      "[  47]  Training error: 0.90248 Test error: 0.90260\n",
      "[  48]  Training error: 0.90248 Test error: 0.90260\n",
      "[  49]  Training error: 0.90248 Test error: 0.90260\n",
      "[  50]  Training error: 0.90248 Test error: 0.90260\n",
      "[  51]  Training error: 0.90085 Test error: 0.89910\n",
      "[  52]  Training error: 0.89782 Test error: 0.89900\n",
      "[  53]  Training error: 0.90070 Test error: 0.89680\n",
      "[  54]  Training error: 0.90248 Test error: 0.90260\n",
      "[  55]  Training error: 0.90965 Test error: 0.91080\n",
      "[  56]  Training error: 0.90248 Test error: 0.90260\n",
      "[  57]  Training error: 0.88763 Test error: 0.88650\n",
      "[  58]  Training error: 0.90263 Test error: 0.90180\n",
      "[  59]  Training error: 0.90965 Test error: 0.91080\n",
      "[  60]  Training error: 0.90085 Test error: 0.89910\n",
      "[  61]  Training error: 0.88763 Test error: 0.88650\n",
      "[  62]  Training error: 0.88763 Test error: 0.88650\n",
      "[  63]  Training error: 0.90263 Test error: 0.90180\n",
      "[  64]  Training error: 0.90128 Test error: 0.90200\n",
      "[  65]  Training error: 0.88763 Test error: 0.88650\n",
      "[  66]  Training error: 0.90085 Test error: 0.89910\n",
      "[  67]  Training error: 0.90085 Test error: 0.89910\n",
      "[  68]  Training error: 0.89782 Test error: 0.89900\n",
      "[  69]  Training error: 0.89558 Test error: 0.89720\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels, eta=0.5 ,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexp\\AppData\\Local\\Temp/ipykernel_15448/371986923.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0]  Training error: 0.90965 Test error: 0.91080\n",
      "[   1]  Training error: 0.90263 Test error: 0.90180\n",
      "[   2]  Training error: 0.90128 Test error: 0.90200\n",
      "[   3]  Training error: 0.90263 Test error: 0.90180\n",
      "[   4]  Training error: 0.88763 Test error: 0.88650\n",
      "[   5]  Training error: 0.89558 Test error: 0.89720\n",
      "[   6]  Training error: 0.90263 Test error: 0.90180\n",
      "[   7]  Training error: 0.88763 Test error: 0.88650\n",
      "[   8]  Training error: 0.90248 Test error: 0.90260\n",
      "[   9]  Training error: 0.89782 Test error: 0.89900\n",
      "[  10]  Training error: 0.90248 Test error: 0.90260\n",
      "[  11]  Training error: 0.90248 Test error: 0.90260\n",
      "[  12]  Training error: 0.90248 Test error: 0.90260\n",
      "[  13]  Training error: 0.90248 Test error: 0.90260\n",
      "[  14]  Training error: 0.90263 Test error: 0.90180\n",
      "[  15]  Training error: 0.90248 Test error: 0.90260\n",
      "[  16]  Training error: 0.89782 Test error: 0.89900\n",
      "[  17]  Training error: 0.90128 Test error: 0.90200\n",
      "[  18]  Training error: 0.90128 Test error: 0.90200\n",
      "[  19]  Training error: 0.90085 Test error: 0.89910\n",
      "[  20]  Training error: 0.88763 Test error: 0.88650\n",
      "[  21]  Training error: 0.90128 Test error: 0.90200\n",
      "[  22]  Training error: 0.90965 Test error: 0.91080\n",
      "[  23]  Training error: 0.90965 Test error: 0.91080\n",
      "[  24]  Training error: 0.89782 Test error: 0.89900\n",
      "[  25]  Training error: 0.90248 Test error: 0.90260\n",
      "[  26]  Training error: 0.90128 Test error: 0.90200\n",
      "[  27]  Training error: 0.89782 Test error: 0.89900\n",
      "[  28]  Training error: 0.90137 Test error: 0.90420\n",
      "[  29]  Training error: 0.90128 Test error: 0.90200\n",
      "[  30]  Training error: 0.90965 Test error: 0.91080\n",
      "[  31]  Training error: 0.89558 Test error: 0.89720\n",
      "[  32]  Training error: 0.88763 Test error: 0.88650\n",
      "[  33]  Training error: 0.90128 Test error: 0.90200\n",
      "[  34]  Training error: 0.90248 Test error: 0.90260\n",
      "[  35]  Training error: 0.90137 Test error: 0.90420\n",
      "[  36]  Training error: 0.90248 Test error: 0.90260\n",
      "[  37]  Training error: 0.90085 Test error: 0.89910\n",
      "[  38]  Training error: 0.90137 Test error: 0.90420\n",
      "[  39]  Training error: 0.90965 Test error: 0.91080\n",
      "[  40]  Training error: 0.90137 Test error: 0.90420\n",
      "[  41]  Training error: 0.89782 Test error: 0.89900\n",
      "[  42]  Training error: 0.90137 Test error: 0.90420\n",
      "[  43]  Training error: 0.90085 Test error: 0.89910\n",
      "[  44]  Training error: 0.90248 Test error: 0.90260\n",
      "[  45]  Training error: 0.90137 Test error: 0.90420\n",
      "[  46]  Training error: 0.90965 Test error: 0.91080\n",
      "[  47]  Training error: 0.90085 Test error: 0.89910\n",
      "[  48]  Training error: 0.90248 Test error: 0.90260\n",
      "[  49]  Training error: 0.89782 Test error: 0.89900\n",
      "[  50]  Training error: 0.90248 Test error: 0.90260\n",
      "[  51]  Training error: 0.90137 Test error: 0.90420\n",
      "[  52]  Training error: 0.90263 Test error: 0.90180\n",
      "[  53]  Training error: 0.90085 Test error: 0.89910\n",
      "[  54]  Training error: 0.90248 Test error: 0.90260\n",
      "[  55]  Training error: 0.90263 Test error: 0.90180\n",
      "[  56]  Training error: 0.90263 Test error: 0.90180\n",
      "[  57]  Training error: 0.90137 Test error: 0.90420\n",
      "[  58]  Training error: 0.90137 Test error: 0.90420\n",
      "[  59]  Training error: 0.90248 Test error: 0.90260\n",
      "[  60]  Training error: 0.89782 Test error: 0.89900\n",
      "[  61]  Training error: 0.90128 Test error: 0.90200\n",
      "[  62]  Training error: 0.90128 Test error: 0.90200\n",
      "[  63]  Training error: 0.90137 Test error: 0.90420\n",
      "[  64]  Training error: 0.90128 Test error: 0.90200\n",
      "[  65]  Training error: 0.90263 Test error: 0.90180\n",
      "[  66]  Training error: 0.89782 Test error: 0.89900\n",
      "[  67]  Training error: 0.90070 Test error: 0.89680\n",
      "[  68]  Training error: 0.90128 Test error: 0.90200\n",
      "[  69]  Training error: 0.89782 Test error: 0.89900\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
